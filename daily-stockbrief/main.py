import asyncio
import json
import os
import re
from datetime import datetime
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError

BASE_URL = "https://stock.mk.co.kr/news/media/infostock"
WEB_DATA_PATH = "../daily-stockbrief-web/public/data"  # Next.js public/data ê²½ë¡œ

# ---------- ìœ í‹¸ ----------
def today_markers() -> list[str]:
    now = datetime.now()
    y = now.year
    m = f"{now.month:02d}"
    d = f"{now.day:02d}"
    return [f"{y}-{m}-{d}", f"{y}.{m}.{d}", f"{y}/{m}/{d}"]

def clean_title(title: str) -> str:
    return re.sub(r"\s*\(ì¦ì‹œìš”ì•½\(\d+\)\)", "", title).strip()

def to_abs(url: str) -> str:
    if not url:
        return ""
    return url if url.startswith("http") else f"https://stock.mk.co.kr{url}"

# ---------- ê³µí†µ: ê¸°ì‚¬ ì°¾ê¸° ----------
async def find_today_article(context, start_page, required_subs: list[str], max_pages: int = 5):
    """ì˜¤ëŠ˜ ë‚ ì§œì˜ ì¦ì‹œìš”ì•½ ê¸°ì‚¬ë¥¼ ì°¾ëŠ”ë‹¤ (ë‚ ì§œ ìˆ«ìë§Œ ë³´ê³  ëŠìŠ¨ ë§¤ì¹­, ê³¼ê±° ê¸°ì‚¬ë§Œ reject)."""
    marks = today_markers()
    today_dash = datetime.now().strftime("%Y-%m-%d")
    page = start_page

    for page_no in range(1, max_pages + 1):
        print(f"[DEBUG] ğŸ” {page_no}í˜ì´ì§€ ê²€ìƒ‰ ì‹œì‘")
        anchors = await page.locator("a").all()
        candidates = []
        for a in anchors:
            try:
                t = (await a.inner_text()).strip()
                if all(s in t for s in required_subs):
                    href = await a.get_attribute("href")
                    if href:
                        candidates.append((t, to_abs(href)))
            except Exception:
                continue

        print(f"[DEBUG] í›„ë³´ ê¸°ì‚¬ {len(candidates)}ê°œ ë°œê²¬")

        for raw_title, url in candidates:
            print(f"[DEBUG] ê²€ì‚¬ì¤‘: {raw_title} ({url})")
            news = await context.new_page()
            try:
                await news.goto(url, wait_until="domcontentloaded", timeout=45000)
                date_loc = news.locator(".news_date, .date, span.date, .info_date")
                date_txt = ""
                try:
                    date_txt = (await date_loc.first.inner_text(timeout=5000)).strip()
                except Exception:
                    pass

                print(f"[DEBUG] ë‚ ì§œí…ìŠ¤íŠ¸: {date_txt}")

                # ë‚ ì§œ ìˆ«ìë§Œ ë½‘ì•„ì„œ ë¹„êµ
                date_digits = re.findall(r"\d+", date_txt)
                date_only = ""
                if len(date_digits) >= 3:
                    date_only = f"{date_digits[0]}-{date_digits[1].zfill(2)}-{date_digits[2].zfill(2)}"

                # ìˆ«ì ì—†ìœ¼ë©´ ë¬´ì¡°ê±´ accept
                if not date_digits:
                    print("[DEBUG] ë‚ ì§œ ìˆ«ì ì—†ìŒ â†’ ê¸°ì‚¬ ì±„íƒ")
                    await news.close()
                    return clean_title(raw_title), url

                # ì˜¤ëŠ˜ ë‚ ì§œë©´ í™•ì • accept
                if date_only == today_dash or any(mark in date_txt for mark in marks):
                    print(f"[INFO] âœ… ì˜¤ëŠ˜ ê¸°ì‚¬ í™•ì • â†’ {raw_title}")
                    await news.close()
                    return clean_title(raw_title), url

                # ê³¼ê±° ê¸°ì‚¬ë©´ reject
                try:
                    article_date = datetime.strptime(date_only, "%Y-%m-%d")
                    today = datetime.strptime(today_dash, "%Y-%m-%d")
                    if article_date < today:
                        print(f"[WARN] ê³¼ê±° ê¸°ì‚¬ â†’ reject: {date_only}")
                        await news.close()
                        continue
                except Exception:
                    print(f"[DEBUG] ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ â†’ ê¸°ì‚¬ ì±„íƒ")
                    await news.close()
                    return clean_title(raw_title), url

                # ë‚ ì§œ ë‹¤ë¥´ì§€ë§Œ ë¯¸ë˜ê±°ë‚˜ íŒŒì‹± ì„±ê³µ â†’ ê·¸ëƒ¥ accept
                print(f"[WARN] ë‚ ì§œ ë¶ˆì¼ì¹˜ ({date_only}), ê·¸ë˜ë„ ê¸°ì‚¬ ì±„íƒ")
                await news.close()
                return clean_title(raw_title), url

            except PlaywrightTimeoutError:
                print(f"[WARN] í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ: {url}")
            finally:
                await news.close()

        # ë‹¤ìŒ í˜ì´ì§€ ì´ë™
        next_btn = page.locator("a.next, a.paging_next, a:has-text('ë‹¤ìŒ')")
        if page_no == max_pages or await next_btn.count() == 0:
            print(f"[DEBUG] ë‹¤ìŒ í˜ì´ì§€ ì—†ìŒ â†’ ê²€ìƒ‰ ì¢…ë£Œ")
            break
        await next_btn.first.click()
        await page.wait_for_load_state("networkidle")  # ajax ë¡œë”© ëŒ€ì‘

    print(f"[WARN] âŒ ì˜¤ëŠ˜ì ê¸°ì‚¬ ëª» ì°¾ìŒ")
    return None, None

# ---------- (6) ìƒí•œê°€/ê¸‰ë“±ì¢…ëª© ----------
async def scrape_gainers(context, url: str, date_str: str):
    page = await context.new_page()
    await page.goto(url, wait_until="domcontentloaded", timeout=60000)

    items = []
    rows = await page.locator("tr").all()
    for row in rows:
        try:
            a = row.locator("a.popup")
            if await a.count() == 0:
                continue
            raw = (await a.first.inner_text()).replace("\r", "").replace("\t", "").strip()
            parts = [p for p in raw.split("\n") if p.strip()]
            name = parts[0].strip()
            code = ""
            if len(parts) > 1:
                m = re.search(r"\((\d{4,6})\)", parts[1])
                if m:
                    code = m.group(1)

            tds = row.locator("td")
            price, change, reason = "", "", ""
            if await tds.count() >= 3:
                try:
                    price = (await tds.nth(0).inner_text()).strip()
                    change = (await tds.nth(1).inner_text()).strip()
                    reason = (await tds.nth(2).inner_text()).strip()
                except Exception:
                    pass

            if name and code and reason:
                items.append({
                    "name": name,
                    "code": code,
                    "price": price,
                    "change": change,
                    "reason": reason
                })
        except Exception:
            continue

    await page.close()
    return [{
        "title": "ìƒí•œê°€/ê¸‰ë“±ì¢…ëª©",
        "url": url,
        "date": date_str,
        "items": items
    }]

# ---------- (3) íŠ¹ì§• í…Œë§ˆ ----------
async def scrape_themes(context, url: str, date_str: str):
    page = await context.new_page()
    await page.goto(url, wait_until="domcontentloaded", timeout=60000)

    body_lines = []
    cells = await page.locator('td[style*="text-align:left"]').all()
    if cells:
        for td in cells:
            try:
                txt = (await td.inner_text()).strip()
                if txt:
                    body_lines.append(txt)
            except Exception:
                continue
    else:
        ps = await page.locator("#news_text p, .news_text p, article p").all()
        for p in ps:
            try:
                txt = (await p.inner_text()).strip()
                if txt:
                    body_lines.append(txt)
            except Exception:
                continue

    await page.close()
    return [{
        "title": "íŠ¹ì§• í…Œë§ˆ",
        "url": url,
        "date": date_str,
        "body": "\n".join(body_lines)
    }]

# ---------- ë©”ì¸ ----------
async def main():
    today_dash = datetime.now().strftime("%Y-%m-%d")
    today_dir = os.path.join(WEB_DATA_PATH, today_dash)
    os.makedirs(today_dir, exist_ok=True)

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context()
        page = await context.new_page()

        # (6) ìƒí•œê°€/ê¸‰ë“±ì¢…ëª©
        await page.goto(BASE_URL, wait_until="load", timeout=60000)
        g_title, g_url = await find_today_article(context, page, ["ì¦ì‹œìš”ì•½(6)"], max_pages=5)
        gainers = []
        if g_url:
            print(f"[INFO] (6) ì˜¤ëŠ˜ ê¸°ì‚¬: {g_title} -> {g_url}")
            gainers = await scrape_gainers(context, g_url, today_dash)
        else:
            print("[WARN] ì˜¤ëŠ˜ì (6) ìƒí•œê°€/ê¸‰ë“± ê¸°ì‚¬ ë¯¸ë°œê²¬")

        # (3) íŠ¹ì§• í…Œë§ˆ
        await page.goto(BASE_URL, wait_until="load", timeout=60000)
        t_title, t_url = await find_today_article(context, page, ["ì¦ì‹œìš”ì•½(3)"], max_pages=5)
        themes = []
        if t_url:
            print(f"[INFO] (3) ì˜¤ëŠ˜ ê¸°ì‚¬: {t_title} -> {t_url}")
            themes = await scrape_themes(context, t_url, today_dash)
        else:
            print("[WARN] ì˜¤ëŠ˜ì (3) íŠ¹ì§• í…Œë§ˆ ê¸°ì‚¬ ë¯¸ë°œê²¬")

        # ì €ì¥
        if gainers and gainers[0]["items"]:
            with open(os.path.join(today_dir, "infostock_gainers.json"), "w", encoding="utf-8") as f:
                json.dump(gainers, f, ensure_ascii=False, indent=2)
            print(f"[SAVE] {len(gainers[0]['items'])}ê°œ (ìƒí•œê°€/ê¸‰ë“±) â†’ {today_dir}")
        else:
            print("[INFO] ìƒí•œê°€/ê¸‰ë“± ë°ì´í„° ì—†ìŒ â†’ ì €ì¥ ìƒëµ")

        if themes and themes[0]["body"]:
            with open(os.path.join(today_dir, "infostock_themes.json"), "w", encoding="utf-8") as f:
                json.dump(themes, f, ensure_ascii=False, indent=2)
            print(f"[SAVE] íŠ¹ì§• í…Œë§ˆ ì €ì¥ ì™„ë£Œ â†’ {today_dir}")
        else:
            print("[INFO] íŠ¹ì§• í…Œë§ˆ ë°ì´í„° ì—†ìŒ â†’ ì €ì¥ ìƒëµ")

        # ìµœì‹  index.json ê°±ì‹ 
        index_path = os.path.join(WEB_DATA_PATH, "index.json")
        with open(index_path, "w", encoding="utf-8") as idx:
            json.dump({"latestDate": today_dash}, idx, ensure_ascii=False, indent=2)
        print(f"[INDEX] latestDate â†’ {today_dash} ê°±ì‹  ì™„ë£Œ")

        await browser.close()

if __name__ == "__main__":
    asyncio.run(main())
